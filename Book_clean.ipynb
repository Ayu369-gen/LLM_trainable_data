{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayu369-gen/LLM_trainable_data/blob/main/Book_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# %%\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "from typing import Optional, List\n",
        "from pathlib import Path\n",
        "# Install pymupdf and pymupdf4llm\n",
        "try:\n",
        "    import fitz # PyMuPDF\n",
        "    from pymupdf4llm import to_markdown\n",
        "except ImportError:\n",
        "    print(\"Installing pymupdf and pymupdf4llm...\")\n",
        "    !pip install pymupdf pymupdf4llm --quiet\n",
        "    import fitz # PyMuPDF\n",
        "    from pymupdf4llm import to_markdown\n",
        "\n",
        "# Install llama_api_client\n",
        "try:\n",
        "    import llama_api_client\n",
        "except ImportError:\n",
        "    print(\"Installing llama_api_client...\")\n",
        "    !pip install llama_api_client --quiet\n",
        "    import llama_api_client\n",
        "\n",
        "from llama_api_client import LlamaAPIClient\n",
        "\n",
        "# No tkinter needed in Colab\n",
        "# import tkinter as tk\n",
        "# from tkinter import filedialog, messagebox\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import files # Import Colab files utility\n",
        "\n",
        "# Set up the Llama API client\n",
        "try:\n",
        "    # IMPORTANT: Replace with your actual Llama API Key or use Colab Secrets\n",
        "    # Never hardcode sensitive keys in shared notebooks.\n",
        "    # os.environ[\"LLAMA_API_KEY\"] = \"LLM|461707323672851|W1xFA7FgVN58q0ep2JFTPyFyTKQ\"\n",
        "    # Recommended way using Colab Secrets:\n",
        "    # from google.colab import userdata\n",
        "    # os.environ[\"LLAMA_API_KEY\"] = userdata.get('LLAMA_API_KEY')\n",
        "\n",
        "    # For this example, keep the hardcoded key for demonstration, but use Secrets in production\n",
        "    os.environ[\"LLAMA_API_KEY\"] = \"YOUR_LLAMA_API\"\n",
        "\n",
        "\n",
        "    api_key = os.environ.get(\"LLAMA_API_KEY\")\n",
        "    if not api_key:\n",
        "         raise ValueError(\"LLAMA_API_KEY not found. Please set it as a Colab Secret or environment variable.\")\n",
        "\n",
        "    client = LlamaAPIClient(\n",
        "        api_key=api_key,\n",
        "        base_url=\"https://api.llama.com/v1/\",\n",
        "    )\n",
        "except ImportError:\n",
        "    raise ImportError(\"llama_api_client is required. Please install it using 'pip install llama_api_client'.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "    print(\"Please set your LLAMA_API_KEY.\")\n",
        "    client = None # Set client to None if key is missing\n",
        "\n",
        "# Function to handle file upload in Colab\n",
        "def upload_pdf_files() -> List[str]:\n",
        "    \"\"\"\n",
        "    Upload PDF files using Google Colab's file upload utility.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of uploaded PDF file paths in the Colab environment.\n",
        "    \"\"\"\n",
        "    print(\"Please upload PDF files using the file upload widget below:\")\n",
        "    uploaded = files.upload() # This opens a file dialog in your browser\n",
        "\n",
        "    uploaded_files = []\n",
        "    for filename in uploaded.keys():\n",
        "        print(f'Uploaded file: {filename}')\n",
        "        uploaded_files.append(filename) # files.upload() saves files to the current directory\n",
        "\n",
        "    if not uploaded_files:\n",
        "        print(\"No files were selected or uploaded.\")\n",
        "        return []\n",
        "\n",
        "    # Optional: Filter for only PDF files if needed, though upload dialog might handle this\n",
        "    # pdf_files_only = [f for f in uploaded_files if f.lower().endswith('.pdf')]\n",
        "    # print(f\"Found {len(pdf_files_only)} PDF files among uploaded.\")\n",
        "    # return pdf_files_only\n",
        "    return uploaded_files\n",
        "\n",
        "\n",
        "# Function to handle file download in Colab\n",
        "def download_processed_files(jsonl_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Download processed JSONL files using Google Colab's file download utility.\n",
        "\n",
        "    Args:\n",
        "        jsonl_dir (str): Directory containing the JSONL files to download.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nPreparing files for download from: {jsonl_dir}\")\n",
        "        jsonl_files = [f for f in os.listdir(jsonl_dir) if f.endswith('.jsonl')]\n",
        "\n",
        "        if not jsonl_files:\n",
        "            print(\"No JSONL files found to download.\")\n",
        "            return\n",
        "\n",
        "        print(\"Downloading processed files...\")\n",
        "        # Colab's files.download handles multiple files, but you can also loop\n",
        "        for file_name in jsonl_files:\n",
        "             file_path = os.path.join(jsonl_dir, file_name)\n",
        "             try:\n",
        "                 files.download(file_path) # Download the file to the user's local machine\n",
        "                 print(f\"Downloaded: {file_name}\")\n",
        "             except Exception as download_error:\n",
        "                 print(f\"Error downloading {file_name}: {download_error}\")\n",
        "\n",
        "\n",
        "        print(\"Download process finished.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error preparing files for download: {e}\")\n",
        "\n",
        "\n",
        "def convert_pdf_to_text(pdf_path: str, output_dir: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a PDF file to markdown text using pymupdf4llm.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        output_dir (str): Directory to save the markdown file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the generated markdown file or None if conversion fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        pdf_name = Path(pdf_path).stem\n",
        "        md_path = os.path.join(output_dir, f\"{pdf_name}_pymupdf4llm.md\")\n",
        "        print(f\"Converting {pdf_path} to markdown...\")\n",
        "        markdown_text = to_markdown(pdf_path)\n",
        "        print(f\"Markdown text length: {len(markdown_text)}\")\n",
        "        with open(md_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_text)\n",
        "        print(f\"Markdown file saved to {md_path}\")\n",
        "        print(\"Markdown file created:\", md_path)\n",
        "        return md_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF to markdown: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text(text: str, max_chunk_size: int = 4000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks of maximum size while trying to keep paragraphs together.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to chunk\n",
        "        max_chunk_size (int): Maximum size of each chunk\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    # Split by paragraphs\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # Check if adding the next paragraph exceeds the max size\n",
        "        # +2 accounts for the potential '\\n\\n' delimiter\n",
        "        if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:\n",
        "            current_chunk += paragraph + '\\n\\n'\n",
        "        else:\n",
        "            # If adding the paragraph exceeds the size, add the current chunk\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            # Start a new chunk with the current paragraph\n",
        "            current_chunk = paragraph + '\\n\\n'\n",
        "\n",
        "    # Add the last accumulated chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def query_llama_api(text: str, prompt: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Send text to the Llama API with a prompt and return the response.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to process (research paper content).\n",
        "        prompt (str): Instructions for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: Processed text or None if an error occurs or client is not initialized.\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        print(\"API client not initialized due to missing API key.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in marine science and aquaculture content processing. Extract ONLY the plain text content related to marine science and aquaculture. Remove all formatting, headers, numbers, and special characters. Output should be clean, continuous text without any structural elements.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\n### Input Text:\\n{text}\"}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            top_p=0.9\n",
        "        )\n",
        "        # Adjusted to access content based on typical API response structure\n",
        "        # The original code used response.completion_message.content.text.strip()\n",
        "        # which might be specific to a custom client library.\n",
        "        # A more standard structure might involve accessing choices and message content.\n",
        "        # Assuming the original structure was correct for your client:\n",
        "        response_text = response.completion_message.content.text.strip()\n",
        "        return response_text\n",
        "    except AttributeError:\n",
        "         print(\"API response structure unexpected. Could not find completion_message.content.text.\")\n",
        "         # Fallback or alternative way to access response text if needed\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying API: {e}\")\n",
        "        # Optional: Add a retry logic here if needed, similar to your original fallback\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_text_chunk(chunk: str, prompt: str) -> Optional[List[str]]:\n",
        "    \"\"\"\n",
        "    Process a single chunk of text through the Llama API and split it into continuous text segments.\n",
        "\n",
        "    Args:\n",
        "        chunk (str): Text chunk to process\n",
        "        prompt (str): Instructions for the LLM\n",
        "\n",
        "    Returns:\n",
        "        Optional[List[str]]: List of processed text segments or None if an error occurs\n",
        "    \"\"\"\n",
        "    cleaned_content = query_llama_api(chunk, prompt)\n",
        "    if cleaned_content is None: # Check for None explicitly\n",
        "        return None\n",
        "\n",
        "    # Post-process to clean up extra whitespace and normalize newlines\n",
        "    cleaned_content = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_content).strip()\n",
        "\n",
        "    # Split content into continuous text segments\n",
        "    segments = []\n",
        "    # Simple split by double newlines to get potential paragraphs from LLM output.\n",
        "    paragraphs = cleaned_content.split('\\n\\n')\n",
        "    for para in paragraphs:\n",
        "        stripped_para = para.strip()\n",
        "        if stripped_para:\n",
        "             # Each non-empty paragraph from the LLM output can be a segment\n",
        "            segments.append(stripped_para)\n",
        "\n",
        "\n",
        "    return segments if segments else None\n",
        "\n",
        "\n",
        "def clean_research_paper(input_file: str, output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Use Llama API to process marine science and aquaculture content while retaining detailed scientific information.\n",
        "    Output is saved in JSONL format with each line as a complete text segment for pretraining.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input text file containing the content.\n",
        "        output_file (str): Path to save the cleaned output text file in JSONL format.\n",
        "    \"\"\"\n",
        "     # Check if the API client is available before proceeding\n",
        "    if client is None:\n",
        "        print(f\"Skipping processing for {input_file} due to missing API key.\")\n",
        "        return\n",
        "\n",
        "    # Read the input file\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file '{input_file}' not found.\")\n",
        "        return\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error: File encoding issue for '{input_file}'. Ensure the file is UTF-8 encoded.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "         print(f\"Error reading input file '{input_file}': {e}\")\n",
        "         return\n",
        "\n",
        "    # Define the prompt for the Llama API\n",
        "    prompt = \"\"\"Extract ONLY the plain text content related to marine science and aquaculture from the following text. Remove all formatting, headers, and structural elements.\n",
        "\n",
        "**Content to Extract:**\n",
        "- Marine biology and ecology\n",
        "- Aquaculture systems and practices\n",
        "- Fisheries management\n",
        "- Marine conservation\n",
        "- Oceanography and marine ecosystems\n",
        "- Marine species biology and behavior\n",
        "- Aquaculture technology and innovations\n",
        "- Marine resource management\n",
        "- Water quality and environmental parameters\n",
        "- Marine food production systems\n",
        "\n",
        "**Output Rules:**\n",
        "1. Output ONLY plain text content.\n",
        "2. Remove all headers, subheaders, and section titles.\n",
        "3. Remove all numerical markings and bullet points.\n",
        "4. Remove all formatting (bold, italic, etc.).\n",
        "5. Remove all special characters and symbols.\n",
        "6. Convert content into continuous paragraphs.\n",
        "7. Maintain proper sentence structure and punctuation.\n",
        "8. Keep all scientific terms and technical descriptions.\n",
        "9. Preserve the meaning and context of the content.\n",
        "10. DO NOT include any meta-instructions or processing commentary.\n",
        "\n",
        "Remember: Your output should be clean, continuous text without any structural elements or formatting.\"\"\"\n",
        "\n",
        "    # Split content into chunks if needed\n",
        "    chunks = chunk_text(content)\n",
        "    all_segments = []\n",
        "\n",
        "    print(f\"Processing {len(chunks)} chunks from {input_file}...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Processing chunk {i}/{len(chunks)}...\")\n",
        "        segments = process_text_chunk(chunk, prompt)\n",
        "        if segments:\n",
        "            all_segments.extend(segments)\n",
        "\n",
        "    if not all_segments:\n",
        "        print(f\"No relevant content extracted from {input_file}. Skipping JSONL creation.\")\n",
        "        return\n",
        "\n",
        "    # Write each segment as a separate JSON object in JSONL format\n",
        "    try:\n",
        "        # Ensure the output directory exists\n",
        "        output_dir = os.path.dirname(output_file)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for segment in all_segments:\n",
        "                json_obj = {\n",
        "                    \"text\": segment\n",
        "                }\n",
        "                file.write(json.dumps(json_obj) + '\\n')\n",
        "        print(f\"Cleaned content saved to '{output_file}' in JSONL format.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to output file '{output_file}': {e}\")\n",
        "\n",
        "def process_multiple_files(input_files: List[str], base_output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Process multiple input files and save the cleaned versions in the output directory.\n",
        "\n",
        "    Args:\n",
        "        input_files (List[str]): List of input file paths within the Colab environment.\n",
        "        base_output_dir (str): Base directory to save the processed files (markdown and jsonl).\n",
        "    \"\"\"\n",
        "    # Create output directories within the base output directory\n",
        "    md_dir = os.path.join(base_output_dir, 'markdown')\n",
        "    jsonl_dir = os.path.join(base_output_dir, 'jsonl')\n",
        "    os.makedirs(md_dir, exist_ok=True)\n",
        "    os.makedirs(jsonl_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Files to process:\", input_files)\n",
        "\n",
        "    if not input_files:\n",
        "        print(\"No files provided to process.\")\n",
        "        return\n",
        "\n",
        "    for input_file in input_files:\n",
        "        input_path = Path(input_file)\n",
        "\n",
        "        # Check if the uploaded file is a PDF\n",
        "        if input_path.suffix.lower() == '.pdf':\n",
        "             # Convert PDF to markdown\n",
        "            md_file = convert_pdf_to_text(input_file, md_dir)\n",
        "            if not md_file:\n",
        "                print(f\"Skipping processing of {input_file} due to PDF conversion error.\")\n",
        "                continue\n",
        "        else:\n",
        "            # If not a PDF, inform the user and skip\n",
        "            print(f\"Skipping non-PDF file: {input_file}. Please upload only PDF files.\")\n",
        "            continue # Skip to the next file\n",
        "\n",
        "        # Process the markdown file\n",
        "        output_file = os.path.join(jsonl_dir, f\"cleaned_{input_path.stem}.jsonl\")\n",
        "        print(f\"\\nProcessing file: {md_file}\")\n",
        "        clean_research_paper(md_file, output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Create timestamp for unique output directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    # Use a directory within the Colab environment, e.g., /content/\n",
        "    base_output_dir = f'/content/cleaned_output_{timestamp}'\n",
        "\n",
        "    # Upload PDF files using Colab's utility and get the list of paths\n",
        "    input_files = upload_pdf_files()\n",
        "\n",
        "    if not input_files:\n",
        "        print(\"No PDF files uploaded. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Process the files\n",
        "    process_multiple_files(input_files, base_output_dir)\n",
        "\n",
        "    # Download the processed JSONL files from the created directory\n",
        "    jsonl_dir = os.path.join(base_output_dir, 'jsonl')\n",
        "    download_processed_files(jsonl_dir) # Use the new download function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check for API key before running main\n",
        "    if os.environ.get(\"LLAMA_API_KEY\"):\n",
        "        main()\n",
        "    else:\n",
        "        print(\"LLAMA_API_KEY is not set. Please set your API key.\")\n",
        "        print(\"Go to 'Secrets' (key icon) in the left sidebar in Google Colab and add 'LLAMA_API_KEY' with your API key.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D-fNUJqW6zck",
        "outputId": "3d1a9b96-a646-4b09-fb5f-05fc37fbc124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload PDF files using the file upload widget below:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c8012f1b-0ac3-4b2a-aeca-ccef5942c5ac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c8012f1b-0ac3-4b2a-aeca-ccef5942c5ac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Marine ecological field methods _ a guide for marine (undergrad & grad).pdf to Marine ecological field methods _ a guide for marine (undergrad & grad).pdf\n",
            "Uploaded file: Marine ecological field methods _ a guide for marine (undergrad & grad).pdf\n",
            "Files to process: ['Marine ecological field methods _ a guide for marine (undergrad & grad).pdf']\n",
            "Converting Marine ecological field methods _ a guide for marine (undergrad & grad).pdf to markdown...\n",
            "Markdown text length: 503230\n",
            "Markdown file saved to /content/cleaned_output_20250531_011831/markdown/Marine ecological field methods _ a guide for marine (undergrad & grad)_pymupdf4llm.md\n",
            "Markdown file created: /content/cleaned_output_20250531_011831/markdown/Marine ecological field methods _ a guide for marine (undergrad & grad)_pymupdf4llm.md\n",
            "\n",
            "Processing file: /content/cleaned_output_20250531_011831/markdown/Marine ecological field methods _ a guide for marine (undergrad & grad)_pymupdf4llm.md\n",
            "Processing 141 chunks from /content/cleaned_output_20250531_011831/markdown/Marine ecological field methods _ a guide for marine (undergrad & grad)_pymupdf4llm.md...\n",
            "Processing chunk 1/141...\n",
            "Processing chunk 2/141...\n",
            "Processing chunk 3/141...\n",
            "Processing chunk 4/141...\n",
            "Processing chunk 5/141...\n",
            "Processing chunk 6/141...\n",
            "Processing chunk 7/141...\n",
            "Processing chunk 8/141...\n",
            "Processing chunk 9/141...\n",
            "Processing chunk 10/141...\n",
            "Processing chunk 11/141...\n",
            "Processing chunk 12/141...\n",
            "Processing chunk 13/141...\n",
            "Processing chunk 14/141...\n",
            "Processing chunk 15/141...\n",
            "Processing chunk 16/141...\n",
            "Processing chunk 17/141...\n",
            "Processing chunk 18/141...\n",
            "Processing chunk 19/141...\n",
            "Processing chunk 20/141...\n",
            "Processing chunk 21/141...\n",
            "Processing chunk 22/141...\n",
            "Processing chunk 23/141...\n",
            "Processing chunk 24/141...\n",
            "Processing chunk 25/141...\n",
            "Processing chunk 26/141...\n",
            "Processing chunk 27/141...\n",
            "Processing chunk 28/141...\n",
            "Processing chunk 29/141...\n",
            "Processing chunk 30/141...\n",
            "Processing chunk 31/141...\n",
            "Processing chunk 32/141...\n",
            "Processing chunk 33/141...\n",
            "Processing chunk 34/141...\n",
            "Processing chunk 35/141...\n",
            "Processing chunk 36/141...\n",
            "Processing chunk 37/141...\n",
            "Processing chunk 38/141...\n",
            "Processing chunk 39/141...\n",
            "Processing chunk 40/141...\n",
            "Processing chunk 41/141...\n",
            "Processing chunk 42/141...\n",
            "Processing chunk 43/141...\n",
            "Processing chunk 44/141...\n",
            "Processing chunk 45/141...\n",
            "Processing chunk 46/141...\n",
            "Processing chunk 47/141...\n",
            "Processing chunk 48/141...\n",
            "Processing chunk 49/141...\n",
            "Processing chunk 50/141...\n",
            "Processing chunk 51/141...\n",
            "Processing chunk 52/141...\n",
            "Processing chunk 53/141...\n",
            "Processing chunk 54/141...\n",
            "Processing chunk 55/141...\n",
            "Processing chunk 56/141...\n",
            "Processing chunk 57/141...\n",
            "Processing chunk 58/141...\n",
            "Processing chunk 59/141...\n",
            "Processing chunk 60/141...\n",
            "Processing chunk 61/141...\n",
            "Processing chunk 62/141...\n",
            "Processing chunk 63/141...\n",
            "Processing chunk 64/141...\n",
            "Processing chunk 65/141...\n",
            "Processing chunk 66/141...\n",
            "Processing chunk 67/141...\n",
            "Processing chunk 68/141...\n",
            "Processing chunk 69/141...\n",
            "Processing chunk 70/141...\n",
            "Processing chunk 71/141...\n",
            "Processing chunk 72/141...\n",
            "Processing chunk 73/141...\n",
            "Processing chunk 74/141...\n",
            "Processing chunk 75/141...\n",
            "Processing chunk 76/141...\n",
            "Processing chunk 77/141...\n",
            "Processing chunk 78/141...\n",
            "Processing chunk 79/141...\n",
            "Processing chunk 80/141...\n",
            "Processing chunk 81/141...\n",
            "Processing chunk 82/141...\n",
            "Processing chunk 83/141...\n",
            "Processing chunk 84/141...\n",
            "Processing chunk 85/141...\n",
            "Processing chunk 86/141...\n",
            "Processing chunk 87/141...\n",
            "Processing chunk 88/141...\n",
            "Processing chunk 89/141...\n",
            "Processing chunk 90/141...\n",
            "Processing chunk 91/141...\n",
            "Processing chunk 92/141...\n",
            "Processing chunk 93/141...\n",
            "Processing chunk 94/141...\n",
            "Processing chunk 95/141...\n",
            "Processing chunk 96/141...\n",
            "Processing chunk 97/141...\n",
            "Processing chunk 98/141...\n",
            "Processing chunk 99/141...\n",
            "Processing chunk 100/141...\n",
            "Processing chunk 101/141...\n",
            "Processing chunk 102/141...\n",
            "Processing chunk 103/141...\n",
            "Processing chunk 104/141...\n",
            "Processing chunk 105/141...\n",
            "Processing chunk 106/141...\n",
            "Processing chunk 107/141...\n",
            "Processing chunk 108/141...\n",
            "Processing chunk 109/141...\n",
            "Processing chunk 110/141...\n",
            "Processing chunk 111/141...\n",
            "Processing chunk 112/141...\n",
            "Processing chunk 113/141...\n",
            "Processing chunk 114/141...\n",
            "Processing chunk 115/141...\n",
            "Processing chunk 116/141...\n",
            "Processing chunk 117/141...\n",
            "Processing chunk 118/141...\n",
            "Processing chunk 119/141...\n",
            "Processing chunk 120/141...\n",
            "Processing chunk 121/141...\n",
            "Processing chunk 122/141...\n",
            "Processing chunk 123/141...\n",
            "Processing chunk 124/141...\n",
            "Processing chunk 125/141...\n",
            "Processing chunk 126/141...\n",
            "Processing chunk 127/141...\n",
            "Processing chunk 128/141...\n",
            "Processing chunk 129/141...\n",
            "Processing chunk 130/141...\n",
            "Processing chunk 131/141...\n",
            "Processing chunk 132/141...\n",
            "Processing chunk 133/141...\n",
            "Processing chunk 134/141...\n",
            "Processing chunk 135/141...\n",
            "Processing chunk 136/141...\n",
            "Processing chunk 137/141...\n",
            "Processing chunk 138/141...\n",
            "Processing chunk 139/141...\n",
            "Processing chunk 140/141...\n",
            "Processing chunk 141/141...\n",
            "Cleaned content saved to '/content/cleaned_output_20250531_011831/jsonl/cleaned_Marine ecological field methods _ a guide for marine (undergrad & grad).jsonl' in JSONL format.\n",
            "\n",
            "Preparing files for download from: /content/cleaned_output_20250531_011831/jsonl\n",
            "Downloading processed files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9cd0227c-598a-41ec-86d5-0727b9bf06a3\", \"cleaned_Marine ecological field methods _ a guide for marine (undergrad & grad).jsonl\", 336889)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: cleaned_Marine ecological field methods _ a guide for marine (undergrad & grad).jsonl\n",
            "Download process finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_api_client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfIxpjNc52bX",
        "outputId": "8a62c09b-a68a-4e70-b101-aacb4b1a457e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_api_client\n",
            "  Downloading llama_api_client-0.1.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->llama_api_client) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->llama_api_client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->llama_api_client) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_api_client) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (0.4.1)\n",
            "Downloading llama_api_client-0.1.1-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llama_api_client\n",
            "Successfully installed llama_api_client-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf4llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CiHX_lV5tqb",
        "outputId": "b0d9eef5-d18d-48a8-db62-4ba4ad7b7479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.0.24-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pymupdf>=1.25.5 (from pymupdf4llm)\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf4llm-0.0.24-py3-none-any.whl (28 kB)\n",
            "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, pymupdf4llm\n",
            "Successfully installed pymupdf-1.26.0 pymupdf4llm-0.0.24\n"
          ]
        }
      ]
    }
  ]
}