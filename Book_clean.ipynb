{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayu369-gen/LLM_trainable_data/blob/main/Book_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_api_client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfIxpjNc52bX",
        "outputId": "8a62c09b-a68a-4e70-b101-aacb4b1a457e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_api_client\n",
            "  Downloading llama_api_client-0.1.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->llama_api_client) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->llama_api_client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->llama_api_client) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_api_client) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (0.4.1)\n",
            "Downloading llama_api_client-0.1.1-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llama_api_client\n",
            "Successfully installed llama_api_client-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf4llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CiHX_lV5tqb",
        "outputId": "b0d9eef5-d18d-48a8-db62-4ba4ad7b7479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.0.24-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pymupdf>=1.25.5 (from pymupdf4llm)\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf4llm-0.0.24-py3-none-any.whl (28 kB)\n",
            "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, pymupdf4llm\n",
            "Successfully installed pymupdf-1.26.0 pymupdf4llm-0.0.24\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# %%\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "from typing import Optional, List\n",
        "from pathlib import Path\n",
        "# Install pymupdf and pymupdf4llm\n",
        "try:\n",
        "    import fitz # PyMuPDF\n",
        "    from pymupdf4llm import to_markdown\n",
        "except ImportError:\n",
        "    print(\"Installing pymupdf and pymupdf4llm...\")\n",
        "    !pip install pymupdf pymupdf4llm --quiet\n",
        "    import fitz # PyMuPDF\n",
        "    from pymupdf4llm import to_markdown\n",
        "\n",
        "# Install llama_api_client\n",
        "try:\n",
        "    import llama_api_client\n",
        "except ImportError:\n",
        "    print(\"Installing llama_api_client...\")\n",
        "    !pip install llama_api_client --quiet\n",
        "    import llama_api_client\n",
        "\n",
        "from llama_api_client import LlamaAPIClient\n",
        "\n",
        "# No tkinter needed in Colab\n",
        "# import tkinter as tk\n",
        "# from tkinter import filedialog, messagebox\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import files # Import Colab files utility\n",
        "\n",
        "# Set up the Llama API client\n",
        "try:\n",
        "    # IMPORTANT: Replace with your actual Llama API Key or use Colab Secrets\n",
        "    # Never hardcode sensitive keys in shared notebooks.\n",
        "    # os.environ[\"LLAMA_API_KEY\"] = \"LLM|461707323672851|W1xFA7FgVN58q0ep2JFTPyFyTKQ\"\n",
        "    # Recommended way using Colab Secrets:\n",
        "    # from google.colab import userdata\n",
        "    # os.environ[\"LLAMA_API_KEY\"] = userdata.get('LLAMA_API_KEY')\n",
        "\n",
        "    # For this example, keep the hardcoded key for demonstration, but use Secrets in production\n",
        "    os.environ[\"LLAMA_API_KEY\"] = \"YOUR_LLAMA_API\"\n",
        "\n",
        "\n",
        "    api_key = os.environ.get(\"LLAMA_API_KEY\")\n",
        "    if not api_key:\n",
        "         raise ValueError(\"LLAMA_API_KEY not found. Please set it as a Colab Secret or environment variable.\")\n",
        "\n",
        "    client = LlamaAPIClient(\n",
        "        api_key=api_key,\n",
        "        base_url=\"https://api.llama.com/v1/\",\n",
        "    )\n",
        "except ImportError:\n",
        "    raise ImportError(\"llama_api_client is required. Please install it using 'pip install llama_api_client'.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "    print(\"Please set your LLAMA_API_KEY.\")\n",
        "    client = None # Set client to None if key is missing\n",
        "\n",
        "# Function to handle file upload in Colab\n",
        "def upload_pdf_files() -> List[str]:\n",
        "    \"\"\"\n",
        "    Upload PDF files using Google Colab's file upload utility.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of uploaded PDF file paths in the Colab environment.\n",
        "    \"\"\"\n",
        "    print(\"Please upload PDF files using the file upload widget below:\")\n",
        "    uploaded = files.upload() # This opens a file dialog in your browser\n",
        "\n",
        "    uploaded_files = []\n",
        "    for filename in uploaded.keys():\n",
        "        print(f'Uploaded file: {filename}')\n",
        "        uploaded_files.append(filename) # files.upload() saves files to the current directory\n",
        "\n",
        "    if not uploaded_files:\n",
        "        print(\"No files were selected or uploaded.\")\n",
        "        return []\n",
        "\n",
        "    # Optional: Filter for only PDF files if needed, though upload dialog might handle this\n",
        "    # pdf_files_only = [f for f in uploaded_files if f.lower().endswith('.pdf')]\n",
        "    # print(f\"Found {len(pdf_files_only)} PDF files among uploaded.\")\n",
        "    # return pdf_files_only\n",
        "    return uploaded_files\n",
        "\n",
        "\n",
        "# Function to handle file download in Colab\n",
        "def download_processed_files(jsonl_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Download processed JSONL files using Google Colab's file download utility.\n",
        "\n",
        "    Args:\n",
        "        jsonl_dir (str): Directory containing the JSONL files to download.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nPreparing files for download from: {jsonl_dir}\")\n",
        "        jsonl_files = [f for f in os.listdir(jsonl_dir) if f.endswith('.jsonl')]\n",
        "\n",
        "        if not jsonl_files:\n",
        "            print(\"No JSONL files found to download.\")\n",
        "            return\n",
        "\n",
        "        print(\"Downloading processed files...\")\n",
        "        # Colab's files.download handles multiple files, but you can also loop\n",
        "        for file_name in jsonl_files:\n",
        "             file_path = os.path.join(jsonl_dir, file_name)\n",
        "             try:\n",
        "                 files.download(file_path) # Download the file to the user's local machine\n",
        "                 print(f\"Downloaded: {file_name}\")\n",
        "             except Exception as download_error:\n",
        "                 print(f\"Error downloading {file_name}: {download_error}\")\n",
        "\n",
        "\n",
        "        print(\"Download process finished.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error preparing files for download: {e}\")\n",
        "\n",
        "\n",
        "def convert_pdf_to_text(pdf_path: str, output_dir: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a PDF file to markdown text using pymupdf4llm.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        output_dir (str): Directory to save the markdown file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the generated markdown file or None if conversion fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        pdf_name = Path(pdf_path).stem\n",
        "        md_path = os.path.join(output_dir, f\"{pdf_name}_pymupdf4llm.md\")\n",
        "        print(f\"Converting {pdf_path} to markdown...\")\n",
        "        markdown_text = to_markdown(pdf_path)\n",
        "        print(f\"Markdown text length: {len(markdown_text)}\")\n",
        "        with open(md_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_text)\n",
        "        print(f\"Markdown file saved to {md_path}\")\n",
        "        print(\"Markdown file created:\", md_path)\n",
        "        return md_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF to markdown: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text(text: str, max_chunk_size: int = 4000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks of maximum size while trying to keep paragraphs together.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to chunk\n",
        "        max_chunk_size (int): Maximum size of each chunk\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    # Split by paragraphs\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # Check if adding the next paragraph exceeds the max size\n",
        "        # +2 accounts for the potential '\\n\\n' delimiter\n",
        "        if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:\n",
        "            current_chunk += paragraph + '\\n\\n'\n",
        "        else:\n",
        "            # If adding the paragraph exceeds the size, add the current chunk\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            # Start a new chunk with the current paragraph\n",
        "            current_chunk = paragraph + '\\n\\n'\n",
        "\n",
        "    # Add the last accumulated chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def query_llama_api(text: str, prompt: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Send text to the Llama API with a prompt and return the response.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to process (research paper content).\n",
        "        prompt (str): Instructions for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: Processed text or None if an error occurs or client is not initialized.\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        print(\"API client not initialized due to missing API key.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in marine science and aquaculture content processing. Extract ONLY the plain text content related to marine science and aquaculture. Remove all formatting, headers, numbers, and special characters. Output should be clean, continuous text without any structural elements.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\n### Input Text:\\n{text}\"}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            top_p=0.9\n",
        "        )\n",
        "        # Adjusted to access content based on typical API response structure\n",
        "        # The original code used response.completion_message.content.text.strip()\n",
        "        # which might be specific to a custom client library.\n",
        "        # A more standard structure might involve accessing choices and message content.\n",
        "        # Assuming the original structure was correct for your client:\n",
        "        response_text = response.completion_message.content.text.strip()\n",
        "        return response_text\n",
        "    except AttributeError:\n",
        "         print(\"API response structure unexpected. Could not find completion_message.content.text.\")\n",
        "         # Fallback or alternative way to access response text if needed\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying API: {e}\")\n",
        "        # Optional: Add a retry logic here if needed, similar to your original fallback\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_text_chunk(chunk: str, prompt: str) -> Optional[List[str]]:\n",
        "    \"\"\"\n",
        "    Process a single chunk of text through the Llama API and split it into continuous text segments.\n",
        "\n",
        "    Args:\n",
        "        chunk (str): Text chunk to process\n",
        "        prompt (str): Instructions for the LLM\n",
        "\n",
        "    Returns:\n",
        "        Optional[List[str]]: List of processed text segments or None if an error occurs\n",
        "    \"\"\"\n",
        "    cleaned_content = query_llama_api(chunk, prompt)\n",
        "    if cleaned_content is None: # Check for None explicitly\n",
        "        return None\n",
        "\n",
        "    # Post-process to clean up extra whitespace and normalize newlines\n",
        "    cleaned_content = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_content).strip()\n",
        "\n",
        "    # Split content into continuous text segments\n",
        "    segments = []\n",
        "    # Simple split by double newlines to get potential paragraphs from LLM output.\n",
        "    paragraphs = cleaned_content.split('\\n\\n')\n",
        "    for para in paragraphs:\n",
        "        stripped_para = para.strip()\n",
        "        if stripped_para:\n",
        "             # Each non-empty paragraph from the LLM output can be a segment\n",
        "            segments.append(stripped_para)\n",
        "\n",
        "\n",
        "    return segments if segments else None\n",
        "\n",
        "\n",
        "def clean_research_paper(input_file: str, output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Use Llama API to process marine science and aquaculture content while retaining detailed scientific information.\n",
        "    Output is saved in JSONL format with each line as a complete text segment for pretraining.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input text file containing the content.\n",
        "        output_file (str): Path to save the cleaned output text file in JSONL format.\n",
        "    \"\"\"\n",
        "     # Check if the API client is available before proceeding\n",
        "    if client is None:\n",
        "        print(f\"Skipping processing for {input_file} due to missing API key.\")\n",
        "        return\n",
        "\n",
        "    # Read the input file\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file '{input_file}' not found.\")\n",
        "        return\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error: File encoding issue for '{input_file}'. Ensure the file is UTF-8 encoded.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "         print(f\"Error reading input file '{input_file}': {e}\")\n",
        "         return\n",
        "\n",
        "    # Define the prompt for the Llama API\n",
        "    prompt = \"\"\"Extract ONLY the plain text content related to marine science and aquaculture from the following text. Remove all formatting, headers, and structural elements.\n",
        "\n",
        "**Content to Extract:**\n",
        "- Marine biology and ecology\n",
        "- Aquaculture systems and practices\n",
        "- Fisheries management\n",
        "- Marine conservation\n",
        "- Oceanography and marine ecosystems\n",
        "- Marine species biology and behavior\n",
        "- Aquaculture technology and innovations\n",
        "- Marine resource management\n",
        "- Water quality and environmental parameters\n",
        "- Marine food production systems\n",
        "\n",
        "**Output Rules:**\n",
        "1. Output ONLY plain text content.\n",
        "2. Remove all headers, subheaders, and section titles.\n",
        "3. Remove all numerical markings and bullet points.\n",
        "4. Remove all formatting (bold, italic, etc.).\n",
        "5. Remove all special characters and symbols.\n",
        "6. Convert content into continuous paragraphs.\n",
        "7. Maintain proper sentence structure and punctuation.\n",
        "8. Keep all scientific terms and technical descriptions.\n",
        "9. Preserve the meaning and context of the content.\n",
        "10. DO NOT include any meta-instructions or processing commentary.\n",
        "\n",
        "Remember: Your output should be clean, continuous text without any structural elements or formatting.\"\"\"\n",
        "\n",
        "    # Split content into chunks if needed\n",
        "    chunks = chunk_text(content)\n",
        "    all_segments = []\n",
        "\n",
        "    print(f\"Processing {len(chunks)} chunks from {input_file}...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Processing chunk {i}/{len(chunks)}...\")\n",
        "        segments = process_text_chunk(chunk, prompt)\n",
        "        if segments:\n",
        "            all_segments.extend(segments)\n",
        "\n",
        "    if not all_segments:\n",
        "        print(f\"No relevant content extracted from {input_file}. Skipping JSONL creation.\")\n",
        "        return\n",
        "\n",
        "    # Write each segment as a separate JSON object in JSONL format\n",
        "    try:\n",
        "        # Ensure the output directory exists\n",
        "        output_dir = os.path.dirname(output_file)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for segment in all_segments:\n",
        "                json_obj = {\n",
        "                    \"text\": segment\n",
        "                }\n",
        "                file.write(json.dumps(json_obj) + '\\n')\n",
        "        print(f\"Cleaned content saved to '{output_file}' in JSONL format.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to output file '{output_file}': {e}\")\n",
        "\n",
        "def process_multiple_files(input_files: List[str], base_output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Process multiple input files and save the cleaned versions in the output directory.\n",
        "\n",
        "    Args:\n",
        "        input_files (List[str]): List of input file paths within the Colab environment.\n",
        "        base_output_dir (str): Base directory to save the processed files (markdown and jsonl).\n",
        "    \"\"\"\n",
        "    # Create output directories within the base output directory\n",
        "    md_dir = os.path.join(base_output_dir, 'markdown')\n",
        "    jsonl_dir = os.path.join(base_output_dir, 'jsonl')\n",
        "    os.makedirs(md_dir, exist_ok=True)\n",
        "    os.makedirs(jsonl_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Files to process:\", input_files)\n",
        "\n",
        "    if not input_files:\n",
        "        print(\"No files provided to process.\")\n",
        "        return\n",
        "\n",
        "    for input_file in input_files:\n",
        "        input_path = Path(input_file)\n",
        "\n",
        "        # Check if the uploaded file is a PDF\n",
        "        if input_path.suffix.lower() == '.pdf':\n",
        "             # Convert PDF to markdown\n",
        "            md_file = convert_pdf_to_text(input_file, md_dir)\n",
        "            if not md_file:\n",
        "                print(f\"Skipping processing of {input_file} due to PDF conversion error.\")\n",
        "                continue\n",
        "        else:\n",
        "            # If not a PDF, inform the user and skip\n",
        "            print(f\"Skipping non-PDF file: {input_file}. Please upload only PDF files.\")\n",
        "            continue # Skip to the next file\n",
        "\n",
        "        # Process the markdown file\n",
        "        output_file = os.path.join(jsonl_dir, f\"cleaned_{input_path.stem}.jsonl\")\n",
        "        print(f\"\\nProcessing file: {md_file}\")\n",
        "        clean_research_paper(md_file, output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Create timestamp for unique output directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    # Use a directory within the Colab environment, e.g., /content/\n",
        "    base_output_dir = f'/content/cleaned_output_{timestamp}'\n",
        "\n",
        "    # Upload PDF files using Colab's utility and get the list of paths\n",
        "    input_files = upload_pdf_files()\n",
        "\n",
        "    if not input_files:\n",
        "        print(\"No PDF files uploaded. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Process the files\n",
        "    process_multiple_files(input_files, base_output_dir)\n",
        "\n",
        "    # Download the processed JSONL files from the created directory\n",
        "    jsonl_dir = os.path.join(base_output_dir, 'jsonl')\n",
        "    download_processed_files(jsonl_dir) # Use the new download function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check for API key before running main\n",
        "    if os.environ.get(\"LLAMA_API_KEY\"):\n",
        "        main()\n",
        "    else:\n",
        "        print(\"LLAMA_API_KEY is not set. Please set your API key.\")\n",
        "        print(\"Go to 'Secrets' (key icon) in the left sidebar in Google Colab and add 'LLAMA_API_KEY' with your API key.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "D-fNUJqW6zck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}