{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayu369-gen/LLM_trainable_data/blob/main/Research_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf4llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFC4Ohz4XeQl",
        "outputId": "0408e969-ca69-4163-83a0-e31d2aa05aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.0.24-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pymupdf>=1.25.5 (from pymupdf4llm)\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf4llm-0.0.24-py3-none-any.whl (28 kB)\n",
            "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, pymupdf4llm\n",
            "Successfully installed pymupdf-1.26.0 pymupdf4llm-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_api_client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-eqt_OYXt1n",
        "outputId": "5de7f3fd-efd2-450f-ee42-5d34b92d0705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_api_client\n",
            "  Downloading llama_api_client-0.1.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from llama_api_client) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->llama_api_client) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->llama_api_client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->llama_api_client) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_api_client) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->llama_api_client) (0.4.1)\n",
            "Downloading llama_api_client-0.1.1-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llama_api_client\n",
            "Successfully installed llama_api_client-0.1.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "from typing import Optional, List\n",
        "from pathlib import Path\n",
        "import fitz  # PyMuPDF\n",
        "from pymupdf4llm import to_markdown\n",
        "# Remove tkinter imports\n",
        "# import tkinter as tk\n",
        "# from tkinter import filedialog, messagebox\n",
        "from datetime import datetime\n",
        "\n",
        "# Import Colab file upload utilities\n",
        "from google.colab import files\n",
        "\n",
        "# Set up the Llama API client\n",
        "try:\n",
        "    from llama_api_client import LlamaAPIClient\n",
        "    os.environ[\"LLAMA_API_KEY\"] = \"YOUR_LLAMA_API\"\n",
        "    client = LlamaAPIClient(\n",
        "        api_key=os.environ.get(\"LLAMA_API_KEY\"),\n",
        "        base_url=\"https://api.llama.com/v1/\",\n",
        "    )\n",
        "except ImportError:\n",
        "    raise ImportError(\"llama_api_client is required. Please install it using 'pip install llama_api_client'.\")\n",
        "\n",
        "# Remove the select_pdf_files function\n",
        "\n",
        "def download_jsonl_files(jsonl_dir: str, download_dir: str) -> None:\n",
        "    \"\"\"Copy processed JSONL files to the download directory and offer download in Colab.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(download_dir, exist_ok=True)\n",
        "        print(f\"\\nCopying files to download directory: {download_dir}\")\n",
        "        for file in os.listdir(jsonl_dir):\n",
        "            if file.endswith('.jsonl'):\n",
        "                src_path = os.path.join(jsonl_dir, file)\n",
        "                dst_path = os.path.join(download_dir, file)\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "        print(f\"Files prepared for download in: {download_dir}\")\n",
        "\n",
        "        # Offer download in Colab\n",
        "        print(\"\\nOffering files for download:\")\n",
        "        for file_name in os.listdir(download_dir):\n",
        "            if file_name.endswith('.jsonl'):\n",
        "                file_path = os.path.join(download_dir, file_name)\n",
        "                try:\n",
        "                    files.download(file_path)\n",
        "                    print(f\" - Downloaded {file_name}\")\n",
        "                except Exception as download_error:\n",
        "                    print(f\" - Failed to download {file_name}: {download_error}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during file processing or download preparation: {e}\")\n",
        "\n",
        "\n",
        "def extract_metadata(pdf_path: str) -> dict:\n",
        "    \"\"\"Extract metadata from PDF (title, authors, year).\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        metadata = doc.metadata\n",
        "        doc.close()\n",
        "\n",
        "        return {\n",
        "            \"title\": metadata.get(\"title\", \"\"),\n",
        "            \"authors\": metadata.get(\"author\", \"\"),\n",
        "            \"creation_date\": metadata.get(\"creationDate\", \"\"),\n",
        "            \"keywords\": metadata.get(\"keywords\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting metadata: {e}\")\n",
        "        return {}\n",
        "\n",
        "def convert_pdf_to_text(pdf_path: str, output_dir: str) -> Optional[str]:\n",
        "    \"\"\"Convert a PDF file to markdown text using pymupdf4llm.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        pdf_name = Path(pdf_path).stem\n",
        "        md_path = os.path.join(output_dir, f\"{pdf_name}_pymupdf4llm.md\")\n",
        "        print(f\"Converting {pdf_path} to markdown...\")\n",
        "        markdown_text = to_markdown(pdf_path)\n",
        "        print(f\"Markdown text length: {len(markdown_text)}\")\n",
        "        with open(md_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_text)\n",
        "        print(f\"Markdown file saved to {md_path}\")\n",
        "        return md_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF to markdown: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text(text: str, max_chunk_size: int = 4000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks while preserving research paper structure.\n",
        "    Attempts to keep sections (Abstract, Methods, etc.) together.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_section = \"\"\n",
        "\n",
        "    # Common research paper sections\n",
        "    section_headers = [\n",
        "        r'^#{1,3}\\s*Abstract\\s*$',\n",
        "        r'^#{1,3}\\s*Introduction\\s*$',\n",
        "        r'^#{1,3}\\s*Methods\\s*$',\n",
        "        r'^#{1,3}\\s*Materials and Methods\\s*$',\n",
        "        r'^#{1,3}\\s*Results\\s*$',\n",
        "        r'^#{1,3}\\s*Discussion\\s*$',\n",
        "        r'^#{1,3}\\s*Conclusion\\s*$',\n",
        "        r'^#{1,3}\\s*References\\s*$',\n",
        "        r'^#{1,3}\\s*Acknowledgements\\s*$'\n",
        "    ]\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        # Check if line is a section header\n",
        "        is_section = any(re.match(header, line, re.IGNORECASE) for header in section_headers)\n",
        "\n",
        "        if is_section:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = line + '\\n'\n",
        "            current_section = line\n",
        "        else:\n",
        "            if len(current_chunk) + len(line) + 1 <= max_chunk_size:\n",
        "                current_chunk += line + '\\n'\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                current_chunk = (current_section + '\\n' + line + '\\n') if current_section else (line + '\\n')\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def query_llama_api(text: str, prompt: str) -> Optional[str]:\n",
        "    \"\"\"Send text to the Llama API with a prompt and return the response.\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in marine science and aquaculture research paper analysis. Extract ONLY the plain text content related to marine science and aquaculture. Remove all formatting, headers, numbers, and special characters. Output should be clean, continuous text without any structural elements.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\n### Input Text:\\n{text}\"}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            top_p=0.9\n",
        "        )\n",
        "        response_text = response.completion_message.content.text.strip()\n",
        "        return response_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying API: {e}\")\n",
        "        try:\n",
        "            # Fallback with default parameters if specific ones cause issues\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert in marine science and aquaculture research paper analysis. Extract ONLY the plain text content related to marine science and aquaculture. Remove all formatting, headers, numbers, and special characters. Output should be clean, continuous text without any structural elements.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"{prompt}\\n\\n### Input Text:\\n{text}\"}\n",
        "                ]\n",
        "            )\n",
        "            response_text = response.completion_message.content.text.strip()\n",
        "            return response_text\n",
        "        except Exception as fallback_e:\n",
        "            print(f\"Fallback API call failed: {fallback_e}\")\n",
        "            return None\n",
        "\n",
        "def process_text_chunk(chunk: str, prompt: str) -> Optional[List[str]]:\n",
        "    \"\"\"Process a single chunk of text through the Llama API.\"\"\"\n",
        "    cleaned_content = query_llama_api(chunk, prompt)\n",
        "    if not cleaned_content:\n",
        "        return None\n",
        "\n",
        "    # Post-process to clean up extra whitespace and normalize newlines\n",
        "    cleaned_content = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_content).strip()\n",
        "\n",
        "    # Split content into continuous text segments\n",
        "    segments = []\n",
        "    current_segment = []\n",
        "\n",
        "    for line in cleaned_content.split('\\n'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Skip reference-like lines (e.g., \"[1]\", \"(Smith et al., 2020)\")\n",
        "        # Be careful with regex to avoid false positives on valid text\n",
        "        if re.match(r'^\\[\\d+\\]$', line) or re.match(r'^\\([^)]+,\\s*\\d{4}\\)$', line): # Improved regex for year citations\n",
        "            continue\n",
        "\n",
        "        current_segment.append(line)\n",
        "\n",
        "    # Add the segment\n",
        "    if current_segment:\n",
        "        segment_text = ' '.join(current_segment)\n",
        "        if segment_text.strip():\n",
        "            segments.append(segment_text)\n",
        "\n",
        "    return segments\n",
        "\n",
        "def clean_research_paper(input_file: str, output_file: str, metadata: dict) -> None:\n",
        "    \"\"\"\n",
        "    Process aquaculture research paper content while retaining scientific information.\n",
        "    Output includes metadata and is saved in JSONL format.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file '{input_file}' not found.\")\n",
        "        return\n",
        "    except UnicodeDecodeError:\n",
        "        print(\"Error: File encoding issue. Ensure the file is UTF-8 encoded.\")\n",
        "        return\n",
        "\n",
        "    prompt = \"\"\"Extract ONLY the plain text content related to marine science and aquaculture from the following research paper text. Remove all formatting, headers, and structural elements.\n",
        "\n",
        "**Content to Extract:**\n",
        "- Marine biology and ecology\n",
        "- Aquaculture systems and practices\n",
        "- Fisheries management\n",
        "- Marine conservation\n",
        "- Oceanography and marine ecosystems\n",
        "- Marine species biology and behavior\n",
        "- Aquaculture technology and innovations\n",
        "- Marine resource management\n",
        "- Water quality and environmental parameters\n",
        "- Marine food production systems\n",
        "- Experimental methods and results\n",
        "- Statistical analyses\n",
        "- Scientific observations and conclusions\n",
        "\n",
        "**Output Rules:**\n",
        "1. Output ONLY plain text content\n",
        "2. Remove all headers, subheaders, and section titles\n",
        "3. Remove all numerical markings, bullet points, and reference citations (e.g., [1], (Smith et al., 2020))\n",
        "4. Remove all formatting (bold, italic, etc.)\n",
        "5. Remove all special characters and symbols except basic punctuation\n",
        "6. Convert content into continuous paragraphs\n",
        "7. Maintain proper sentence structure and punctuation\n",
        "8. Preserve all scientific terms, technical descriptions, and data\n",
        "9. Preserve the meaning and context of the content\n",
        "10. Exclude reference lists, acknowledgments, and non-scientific content\n",
        "11. DO NOT include any meta-instructions or processing commentary\n",
        "12. Preserve numerical data and statistical results\n",
        "13. Maintain methodology descriptions\n",
        "14. Keep species names and technical terms intact\n",
        "\n",
        "Remember: Your output should be clean, continuous text without any structural elements or formatting.\"\"\"\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    all_segments = []\n",
        "\n",
        "    print(f\"Processing {len(chunks)} chunks from {input_file}...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Processing chunk {i}/{len(chunks)}...\")\n",
        "        segments = process_text_chunk(chunk, prompt)\n",
        "        if segments:\n",
        "            all_segments.extend(segments)\n",
        "\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for segment in all_segments:\n",
        "                json_obj = {\n",
        "                    \"text\": segment,\n",
        "                    \"metadata\": metadata\n",
        "                }\n",
        "                file.write(json.dumps(json_obj) + '\\n')\n",
        "        print(f\"Cleaned content saved to '{output_file}' in JSONL format.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to output file: {e}\")\n",
        "\n",
        "def process_multiple_files(input_files: List[str], output_dir: str) -> None:\n",
        "    \"\"\"Process multiple input files and save the cleaned versions.\"\"\"\n",
        "    md_dir = os.path.join(output_dir, 'markdown')\n",
        "    jsonl_dir = os.path.join(output_dir, 'jsonl')\n",
        "    os.makedirs(md_dir, exist_ok=True)\n",
        "    os.makedirs(jsonl_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Selected files:\", input_files)\n",
        "\n",
        "    for input_file in input_files:\n",
        "        input_path = Path(input_file)\n",
        "\n",
        "        # Extract metadata\n",
        "        metadata = extract_metadata(input_file) if input_path.suffix.lower() == '.pdf' else {}\n",
        "\n",
        "        # Convert PDF to markdown\n",
        "        if input_path.suffix.lower() == '.pdf':\n",
        "            md_file = convert_pdf_to_text(input_file, md_dir)\n",
        "            if not md_file:\n",
        "                print(f\"Skipping {input_file} due to conversion error\")\n",
        "                continue\n",
        "        else:\n",
        "            # If the input file is already markdown, just use it\n",
        "            md_file = input_file\n",
        "\n",
        "        # Process the markdown file\n",
        "        output_file = os.path.join(jsonl_dir, f\"cleaned_{input_path.stem}.jsonl\")\n",
        "        print(f\"\\nProcessing file: {md_file}\")\n",
        "        clean_research_paper(md_file, output_file, metadata)\n",
        "\n",
        "def main():\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = f'cleaned_output_{timestamp}'\n",
        "    jsonl_dir = os.path.join(output_dir, 'jsonl')\n",
        "    download_dir = os.path.join(\"downloads\", f\"processed_jsonl_{timestamp}\") # Use a relative path for Colab\n",
        "\n",
        "    print(\"Please upload your PDF files.\")\n",
        "    # Use Colab's file upload widget\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # 'uploaded' is a dictionary {filename: content}\n",
        "    # We need to save these files to the Colab environment first\n",
        "    input_files = []\n",
        "    upload_dir = \"uploaded_pdfs\" # Directory to save uploaded files\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No files uploaded. Exiting...\")\n",
        "        return\n",
        "\n",
        "    for filename, content in uploaded.items():\n",
        "        file_path = os.path.join(upload_dir, filename)\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "        input_files.append(file_path)\n",
        "        print(f\"Saved uploaded file: {file_path}\")\n",
        "\n",
        "\n",
        "    if not input_files:\n",
        "        print(\"No files found in the upload directory. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Process the uploaded files\n",
        "    process_multiple_files(input_files, output_dir)\n",
        "\n",
        "    # Offer the resulting JSONL files for download via Colab's download function\n",
        "    download_jsonl_files(jsonl_dir, download_dir)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9M-bhmt2Km6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}